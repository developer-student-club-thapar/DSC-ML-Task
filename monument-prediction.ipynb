{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-02T06:04:36.439422Z","iopub.status.busy":"2023-02-02T06:04:36.439065Z","iopub.status.idle":"2023-02-02T06:04:36.445667Z","shell.execute_reply":"2023-02-02T06:04:36.444686Z","shell.execute_reply.started":"2023-02-02T06:04:36.439390Z"},"trusted":true,"id":"QZsEmwrWoTU7"},"outputs":[],"source":["# Necessary Imports\n","import cv2\n","import pathlib\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import os\n","# View an image\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","import random\n","from PIL import Image"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-02T06:04:37.719239Z","iopub.status.busy":"2023-02-02T06:04:37.718881Z","iopub.status.idle":"2023-02-02T06:04:37.726118Z","shell.execute_reply":"2023-02-02T06:04:37.725030Z","shell.execute_reply.started":"2023-02-02T06:04:37.719208Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"5XREgkC8oTU9","executionInfo":{"status":"ok","timestamp":1698569643898,"user_tz":-330,"elapsed":13,"user":{"displayName":"","userId":""}},"outputId":"b18cab7f-2570-4d10-d108-174a08c8783f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Num GPUs Available:  0\n"]}],"source":["print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-02T06:04:38.415248Z","iopub.status.busy":"2023-02-02T06:04:38.414892Z","iopub.status.idle":"2023-02-02T06:04:38.462112Z","shell.execute_reply":"2023-02-02T06:04:38.461065Z","shell.execute_reply.started":"2023-02-02T06:04:38.415218Z"},"trusted":true,"id":"G76u314ooTU-"},"outputs":[],"source":["# Path to Kaggle Input\n","path = \"D:/monument-prediction/Indian-monuments/images\"\n","# Walk through the directory and list number of files\n","for dirpath, dirnames, filenames in os.walk(path):\n","  print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-02T06:04:39.236918Z","iopub.status.busy":"2023-02-02T06:04:39.236216Z","iopub.status.idle":"2023-02-02T06:04:39.243594Z","shell.execute_reply":"2023-02-02T06:04:39.242638Z","shell.execute_reply.started":"2023-02-02T06:04:39.236880Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"Em3qNkTtoTU-","executionInfo":{"status":"ok","timestamp":1698573792889,"user_tz":-330,"elapsed":678,"user":{"displayName":"","userId":""}},"outputId":"6caa27ec-02b5-459d-dc4a-ea897ff4bebb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["('D:/monument-prediction/Indian-monuments/images/train/',\n"," 'D:/monument-prediction/Indian-monuments/images/test/')"]},"metadata":{},"execution_count":106}],"source":["# append the training and the testing paths to the original path\n","train_dir =  path + \"/train/\"\n","test_dir = path + \"/test/\"\n","train_dir, test_dir"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-02T06:04:39.999434Z","iopub.status.busy":"2023-02-02T06:04:39.999074Z","iopub.status.idle":"2023-02-02T06:04:40.008743Z","shell.execute_reply":"2023-02-02T06:04:40.007590Z","shell.execute_reply.started":"2023-02-02T06:04:39.999404Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"ltuBY0D2oTU_","executionInfo":{"status":"ok","timestamp":1698573796054,"user_tz":-330,"elapsed":447,"user":{"displayName":"","userId":""}},"outputId":"8eac1a6a-5451-4c77-a9d8-027160ff4a93"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([], dtype=float64)"]},"metadata":{},"execution_count":107}],"source":["# get all the class names\n","data_dir = pathlib.Path(train_dir)\n","class_names = np.array(sorted([item.name for item in data_dir.glob(\"*\")]))\n","class_names"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-02T06:04:40.841467Z","iopub.status.busy":"2023-02-02T06:04:40.841090Z","iopub.status.idle":"2023-02-02T06:04:40.852661Z","shell.execute_reply":"2023-02-02T06:04:40.851677Z","shell.execute_reply.started":"2023-02-02T06:04:40.841433Z"},"trusted":true,"id":"vjoxs1ZToTU_"},"outputs":[],"source":["# function used to view an random image\n","def view_random_image(target_dir, target_class):\n","    target_folder = target_dir  + target_class\n","\n","  # Get a random image path\n","    random_image = random.sample(os.listdir(target_folder), 1)\n","\n","  # Read in the image and plot it using matplotlib\n","\n","\n","    img = mpimg.imread(target_folder + \"/\" + random_image[0])\n","    imggs=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) #converting the image to grayscale\n","    (thresh, imgbw)=cv2.threshold(imggs, 127, 255, cv2.THRESH_BINARY) #applying threshold operation to convert to balck and white\n","    plt.imshow(img)\n","    plt.title(target_class)\n","    plt.axis(\"off\");\n","\n","    print(f\"Image shape: {img.shape}\") # show the shape of the image\n","\n","    return img\n","\n","\n","# def to_grayscale_from_rgb(img):\n","#     img = tf.image.rgb_to_grayscale(img)\n","#     return img\n","\n","\n","def plot_loss_curves(history):\n","  \"\"\"\n","  Returns separate loss curves for training and validation metrics.\n","  \"\"\"\n","  loss = history.history['loss']\n","  val_loss = history.history['val_loss']\n","\n","  accuracy = history.history['accuracy']\n","  val_accuracy = history.history['val_accuracy']\n","\n","  epochs = range(len(history.history['loss']))\n","\n","  # Plot loss\n","  plt.plot(epochs, loss, label='training_loss')\n","  plt.plot(epochs, val_loss, label='val_loss')\n","  plt.title('Loss')\n","  plt.xlabel('Epochs')\n","  plt.legend()\n","\n","  # Plot accuracy\n","  plt.figure()\n","  plt.plot(epochs, accuracy, label='training_accuracy')\n","  plt.plot(epochs, val_accuracy, label='val_accuracy')\n","  plt.title('Accuracy')\n","  plt.xlabel('Epochs')\n","  plt.legend();"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-02T06:04:42.072024Z","iopub.status.busy":"2023-02-02T06:04:42.071657Z","iopub.status.idle":"2023-02-02T06:04:42.283055Z","shell.execute_reply":"2023-02-02T06:04:42.282164Z","shell.execute_reply.started":"2023-02-02T06:04:42.071993Z"},"trusted":true,"id":"HEtOgffCoTU_"},"outputs":[],"source":[" #View a random image from the training dataset\n","import random\n","img = view_random_image(target_dir=train_dir, target_class=random.choice(class_names)) # get a random class name"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-02T06:05:02.624791Z","iopub.status.busy":"2023-02-02T06:05:02.624411Z","iopub.status.idle":"2023-02-02T06:05:02.947185Z","shell.execute_reply":"2023-02-02T06:05:02.946152Z","shell.execute_reply.started":"2023-02-02T06:05:02.624757Z"},"trusted":true,"id":"UzzkYMLXoTU_"},"outputs":[],"source":["from keras.preprocessing.image import ImageDataGenerator\n","from keras.models import Sequential\n","from keras.layers import Conv2D, MaxPool2D, Flatten, Dense\n","\n","# Rescale the data and create data generator instances\n","train_datagen = ImageDataGenerator(rescale=1/255,)\n","test_datagen = ImageDataGenerator(rescale=1/255,)\n","\n","\n","\n","\n","# Load data in from directories and turn it into batches\n","train_data = train_datagen.flow_from_directory(train_dir,\n","                                               target_size=(300, 300),\n","                                               batch_size=16,\n","                                               class_mode='categorical')\n","\n","test_data = test_datagen.flow_from_directory(test_dir,\n","                                              target_size=(300, 300),\n","                                              batch_size=16,\n","                                              class_mode='categorical')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-02T06:05:05.346224Z","iopub.status.busy":"2023-02-02T06:05:05.345863Z","iopub.status.idle":"2023-02-02T06:05:05.422206Z","shell.execute_reply":"2023-02-02T06:05:05.421275Z","shell.execute_reply.started":"2023-02-02T06:05:05.346192Z"},"trusted":true,"id":"naa_hdvmoTVA"},"outputs":[],"source":["# from tensorflow.keras.models import Sequential\n","# from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense\n","\n","# Create our model\n","model_1 = Sequential([\n","  Conv2D(10, 5, activation='relu', input_shape=(300, 300, 3)),\n","  Conv2D(10, 5, activation='relu'),\n","  MaxPool2D(),\n","  Conv2D(10, 5, activation='relu'),\n","  Conv2D(10, 5, activation='relu'),\n","  MaxPool2D(),\n","  Conv2D(10, 5, activation='relu'),\n","  Conv2D(10, 5, activation='relu'),\n","#   MaxPool2D(),\n","#   Conv2D(10, 5, activation='relu'),\n","#   Conv2D(10, 5, activation='relu'),\n","  Flatten(),\n","  Dense(128, activation='relu'),\n","  Dense(23, activation='softmax')\n","])\n","\n","# Compile the model\n","model_1.compile(loss=\"categorical_crossentropy\",\n","                optimizer=tf.keras.optimizers.Adam(),\n","                metrics=[\"accuracy\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-02T06:05:08.807631Z","iopub.status.busy":"2023-02-02T06:05:08.807252Z","iopub.status.idle":"2023-02-02T06:31:33.259163Z","shell.execute_reply":"2023-02-02T06:31:33.258079Z","shell.execute_reply.started":"2023-02-02T06:05:08.807579Z"},"trusted":true,"id":"LkbCIZdOoTVA"},"outputs":[],"source":["# Fit the model\n","history_1 = model_1.fit(train_data,\n","                        epochs=25,\n","                        steps_per_epoch=len(train_data),\n","                        validation_data=test_data,\n","                        validation_steps=len(test_data))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-02T15:15:54.989947Z","iopub.status.busy":"2023-02-02T15:15:54.989217Z","iopub.status.idle":"2023-02-02T15:15:55.067843Z","shell.execute_reply":"2023-02-02T15:15:55.066522Z","shell.execute_reply.started":"2023-02-02T15:15:54.989828Z"},"trusted":true,"id":"HiDjB3KtoTVA"},"outputs":[],"source":["model_1.evaluate(test_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-31T17:33:23.296534Z","iopub.status.busy":"2023-01-31T17:33:23.296121Z","iopub.status.idle":"2023-01-31T17:33:23.318729Z","shell.execute_reply":"2023-01-31T17:33:23.317423Z","shell.execute_reply.started":"2023-01-31T17:33:23.296500Z"},"trusted":true,"id":"ca0fA9N7oTVB"},"outputs":[],"source":["model_1.save_weights(\"save_trained_weights.h5\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-31T12:38:53.258919Z","iopub.status.busy":"2023-01-31T12:38:53.258561Z","iopub.status.idle":"2023-01-31T12:38:53.687758Z","shell.execute_reply":"2023-01-31T12:38:53.686658Z","shell.execute_reply.started":"2023-01-31T12:38:53.258888Z"},"trusted":true,"id":"zdoLC4XYoTVB"},"outputs":[],"source":["plot_loss_curves(history_1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-26T12:41:00.655929Z","iopub.status.busy":"2023-01-26T12:41:00.655506Z","iopub.status.idle":"2023-01-26T12:41:00.666269Z","shell.execute_reply":"2023-01-26T12:41:00.664950Z","shell.execute_reply.started":"2023-01-26T12:41:00.655894Z"},"trusted":true,"id":"ZcBv-9hkoTVB"},"outputs":[],"source":["# Create a function to import an image and resize it to be able to be used with our model\n","def load_and_prep_image(filename, img_shape=300):\n","  \"\"\"\n","  Reads an image from filename, turns it into a tensor\n","  and reshapes it to (img_shape, img_shape, colour_channel).\n","  \"\"\"\n","  # Read in target file (an image)\n","  img = tf.io.read_file(filename)\n","\n","  # Decode the read file into a tensor & ensure 3 colour channels\n","  # (our model is trained on images with 3 colour channels and sometimes images have 4 colour channels)\n","  img = tf.image.decode_image(img, channels=3)\n","\n","  # Resize the image (to the same size our model was trained on)\n","  img = tf.image.resize(img, size = [img_shape, img_shape])\n","\n","  # Rescale the image (get all values between 0 and 1)\n","  img = img/255.\n","\n","  return img\n","\n","# Adjust function to work with multi-class\n","def pred_and_plot(model, filename, class_names):\n","  \"\"\"\n","  Imports an image located at filename, makes a prediction on it with\n","  a trained model and plots the image with the predicted class as the title.\n","  \"\"\"\n","  # Import the target image and preprocess it\n","  img = load_and_prep_image(filename)\n","\n","  # Make a prediction\n","  pred = model.predict(tf.expand_dims(img, axis=0))\n","\n","  # Get the predicted class\n","  if len(pred[0]) > 1: # check for multi-class\n","    pred_class = class_names[pred.argmax()] # if more than one output, take the max\n","  else:\n","    pred_class = class_names[int(tf.round(pred)[0][0])] # if only one output, round\n","\n","  # Plot the image and predicted class\n","  plt.imshow(img)\n","  plt.title(f\"Prediction: {pred_class}\")\n","  plt.axis(False);"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-26T12:41:06.597916Z","iopub.status.busy":"2023-01-26T12:41:06.597472Z","iopub.status.idle":"2023-01-26T12:41:06.958028Z","shell.execute_reply":"2023-01-26T12:41:06.956886Z","shell.execute_reply.started":"2023-01-26T12:41:06.597882Z"},"trusted":true,"id":"8R82RudqoTVB"},"outputs":[],"source":["# make a new prediction\n","pred_and_plot(model_1, \"D:/monument-prediction/Indian-monuments/images/test/charminar/35.jpg\", class_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-26T12:41:11.921510Z","iopub.status.busy":"2023-01-26T12:41:11.920821Z","iopub.status.idle":"2023-01-26T12:41:11.987450Z","shell.execute_reply":"2023-01-26T12:41:11.986577Z","shell.execute_reply.started":"2023-01-26T12:41:11.921470Z"},"trusted":true,"id":"5wtoGMKjoTVB"},"outputs":[],"source":["# save the model\n","model_1.save(\"monument-model\")"]},{"cell_type":"markdown","source":["# **Creating an interactive UI for the Model**"],"metadata":{"id":"lyrIuKdLGnxx"}},{"cell_type":"markdown","source":["*Before executing the below code blocks, please execute the first block of the file (Necessary Imports)*"],"metadata":{"id":"xSBLVo8fLTkV"}},{"cell_type":"markdown","source":["# **Gradio**\n","Gradio is an Python library that simplifies and accelerates the process of building interactive machine learning and interfaces. It provides an easy-to-use platform for creating web-based UIs for your machine learning models, allowing users to interact with and make predictions using those models without needing any programming experience."],"metadata":{"id":"cfpdkD23D1Zc"}},{"cell_type":"code","source":["# Installing gradio\n","!pip install gradio"],"metadata":{"id":"omJvpXiaol2c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698578471333,"user_tz":-330,"elapsed":29525,"user":{"displayName":"Akshat Bakshi","userId":"00344908296219908495"}},"outputId":"76f7623d-38dc-4900-b3f0-05e793ad81c8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gradio\n","  Downloading gradio-3.50.2-py3-none-any.whl (20.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio)\n","  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n","Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n","Collecting fastapi (from gradio)\n","  Downloading fastapi-0.104.0-py3-none-any.whl (92 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ffmpy (from gradio)\n","  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting gradio-client==0.6.1 (from gradio)\n","  Downloading gradio_client-0.6.1-py3-none-any.whl (299 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting httpx (from gradio)\n","  Downloading httpx-0.25.0-py3-none-any.whl (75 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.7/75.7 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface-hub>=0.14.0 (from gradio)\n","  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.1.0)\n","Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.2)\n","Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.3)\n","Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n","Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.23.5)\n","Collecting orjson~=3.0 (from gradio)\n","  Downloading orjson-3.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2)\n","Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.3)\n","Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.10.13)\n","Collecting pydub (from gradio)\n","  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n","Collecting python-multipart (from gradio)\n","  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n","Requirement already satisfied: requests~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.31.0)\n","Collecting semantic-version~=2.0 (from gradio)\n","  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n","Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.5.0)\n","Collecting uvicorn>=0.14.0 (from gradio)\n","  Downloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting websockets<12.0,>=10.0 (from gradio)\n","  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.6.1->gradio) (2023.6.0)\n","Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.1)\n","Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (3.12.4)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (4.66.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.1.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.43.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.3.post1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (3.3.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (2023.7.22)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (8.1.7)\n","Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio)\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (3.7.1)\n","Collecting starlette<0.28.0,>=0.27.0 (from fastapi->gradio)\n","  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting typing-extensions~=4.0 (from gradio)\n","  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n","Collecting httpcore<0.19.0,>=0.18.0 (from httpx->gradio)\n","  Downloading httpcore-0.18.0-py3-none-any.whl (76 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.3.0)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi->gradio) (1.1.3)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.1.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.7.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.30.2)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.10.6)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n","Building wheels for collected packages: ffmpy\n","  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=4c422490d3797de09e9e9c359db895897fc77373b673754a8c94b9f4a867f2f6\n","  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\n","Successfully built ffmpy\n","Installing collected packages: pydub, ffmpy, websockets, typing-extensions, semantic-version, python-multipart, orjson, h11, aiofiles, uvicorn, starlette, huggingface-hub, httpcore, httpx, fastapi, gradio-client, gradio\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing_extensions 4.5.0\n","    Uninstalling typing_extensions-4.5.0:\n","      Successfully uninstalled typing_extensions-4.5.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","lida 0.0.10 requires kaleido, which is not installed.\n","tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed aiofiles-23.2.1 fastapi-0.104.0 ffmpy-0.3.1 gradio-3.50.2 gradio-client-0.6.1 h11-0.14.0 httpcore-0.18.0 httpx-0.25.0 huggingface-hub-0.18.0 orjson-3.9.10 pydub-0.25.1 python-multipart-0.0.6 semantic-version-2.10.0 starlette-0.27.0 typing-extensions-4.8.0 uvicorn-0.23.2 websockets-11.0.3\n"]}]},{"cell_type":"code","source":["# Cloning the repository to get access to data\n","!git clone https://github.com/developer-student-club-thapar/DSC-ML-Task"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WUTBk4TBooUQ","executionInfo":{"status":"ok","timestamp":1698578510060,"user_tz":-330,"elapsed":33344,"user":{"displayName":"Akshat Bakshi","userId":"00344908296219908495"}},"outputId":"acc343ef-08b6-4f1d-8a78-366e481b8fe0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'DSC-ML-Task'...\n","remote: Enumerating objects: 3751, done.\u001b[K\n","remote: Counting objects: 100% (15/15), done.\u001b[K\n","remote: Compressing objects: 100% (15/15), done.\u001b[K\n","remote: Total 3751 (delta 8), reused 1 (delta 0), pack-reused 3736\u001b[K\n","Receiving objects: 100% (3751/3751), 577.95 MiB | 22.30 MiB/s, done.\n","Resolving deltas: 100% (44/44), done.\n","Updating files: 100% (4781/4781), done.\n"]}]},{"cell_type":"code","source":["%cd DSC-ML-Task/"],"metadata":{"id":"05caHx0tqyF9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698578518315,"user_tz":-330,"elapsed":414,"user":{"displayName":"Akshat Bakshi","userId":"00344908296219908495"}},"outputId":"7d780e36-e549-4006-b741-2f5ad0972840"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/DSC-ML-Task\n"]}]},{"cell_type":"code","source":["# Load your trained model\n","model_path = \"/content/DSC-ML-Task/saved_trained_model\"\n","model = tf.keras.models.load_model(model_path)"],"metadata":{"id":"raLbAXmyF_SA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Getting the class names\n","train_dir = \"/content/DSC-ML-Task/Indian-monuments/images/train\"  # Update with the path to your training directory\n","data_dir = pathlib.Path(train_dir)\n","class_names = np.array(sorted([item.name for item in data_dir.glob(\"*\")]))"],"metadata":{"id":"txYFNSPvGIeD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define a function for making predictions\n","def predict_image(input_image):\n","    # Preprocess the input image\n","    img = input_image\n","\n","    # Resize the input image to match the model's expected shape (300x300)\n","    img = tf.image.resize(img, [300, 300])\n","\n","    # Normalize the image\n","    img = img / 255.0\n","\n","    # Make a prediction\n","    pred = model.predict(np.expand_dims(img, axis=0))\n","\n","    # Get the predicted class\n","    predicted_class_index = np.argmax(pred)\n","\n","    # Map the class index to a string label (replace with your class labels)\n","    predicted_class_label = class_names[predicted_class_index]\n","\n","    return predicted_class_label"],"metadata":{"id":"8-uN8G_FrVtA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gradio as gr\n","\n","# Create Gradio input component\n","input_component = gr.inputs.Image(type=\"numpy\")\n","\n","# Create a Gradio output component to display the predicted class label\n","output_component = gr.outputs.Label(type=\"text\", label=\"Predicted Class Label\")\n","\n","# Create a Gradio interface\n","gr.Interface(fn=predict_image, inputs=input_component, outputs=output_component).launch()"],"metadata":{"id":"s-wclOR74ATQ"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.7"},"colab":{"provenance":[{"file_id":"https://github.com/developer-student-club-thapar/DSC-ML-Task/blob/main/monument-prediction.ipynb","timestamp":1698577993195}]}},"nbformat":4,"nbformat_minor":0}